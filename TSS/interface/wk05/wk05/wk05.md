# Interface - Wk05

[Back](../../interface.md)

- [Interface - Wk05](#interface---wk05)
  - [Heuristic Analysis](#heuristic-analysis)
    - [When to perform an Analysis](#when-to-perform-an-analysis)
    - [Deliverables](#deliverables)
    - [Evaluation Process](#evaluation-process)
      - [Define the scope](#define-the-scope)
      - [Know the business requirements and the users](#know-the-business-requirements-and-the-users)
      - [Decide which reporting tools and heuristics](#decide-which-reporting-tools-and-heuristics)
      - [Evaluate the experience and usability issues](#evaluate-the-experience-and-usability-issues)
      - [Analyze, aggregate, and present the results.](#analyze-aggregate-and-present-the-results)
      - [The analysis output](#the-analysis-output)
      - [Analysis output](#analysis-output)

---

## Heuristic Analysis

- **Usability Evaluation**

  - It’s not enough to design a nice-looking product; it also has to be **usable**
  - Well-designed products **have excellent usability**, and because **usability** is a significant contributor to **product quality**, it elevates the **user experience**.

- `heuristic analysis`
  - used to **identify a product’s common usability issues** so that the problems can be resolved, consequently **improving the user’s satisfaction and experience** and raising the chances of a digital product’s **success** overall.
  - an evaluation method in which one or more **experts compare a product’s design** to a **list** of predefined design principles (heuristics) and **identify** where the product is **not following** those principles.
    - “Heuristic evaluation involves having a small **set of evaluators examine** the interface and judge its compliance with recognized usability principles” (the ‘heuristics’). — Jakob Nielsen

---

- During the evaluation, **individual evaluators** assign a “**severity rating**” to each of the **usability issues** identified.
- As a rule, **UX designers** work their way down **from the most critical issues** on the backlog **to the least critical**.
- It is typical for the **design team** to give issues with the **highest severity rating the most attention** where designing and implementing the interface.
- Even though a single experienced UX evaluator is usually adept at identifying the most critical usability issues, a **group of evaluators** is generally the **best** option.
- Between 5 and 8 individuals recommended:
  - They should be able to flag over **80**% of usability problems.

---

### When to perform an Analysis

- A heuristic analysis can be performed **at any advanced stage** of the **design** process
- With **new** products, a `heuristic analysis` is usually performed **later in the design phase**
  - after **wireframing** and **prototyping** and before **visual design** and **UI development** begins.
  - Do it **too late** and making changes will become **costly**.
- **Existing** products found to have poor usability will often have a heuristic analysis run on them **before** a **redesign** begins.

---

### Deliverables

- As with other usability tests or inspection methods, the typical deliverable is a `consolidated report` which not only **identifies usability issues**, but **ranks** them on a scale from severe to mildly problematic.
- A heuristic evaluation report **doesn’t include solutions**
  - fortunately, many usability problems have fairly obvious fixes, and once identified the design team can start working on them.

---

### Evaluation Process

- Following an established set of **steps** ensures that a heuristic analysis will run efficiently and yield maximum results.
- Here’s a heuristic analysis **checklist**:
  - 1. Define the **scope**.
  - 2. Know the business **requirements** and **demographic** of the end-users.
  - 3. Decide on which **reporting tools** and heuristics to use.
  - 4. **Evaluate** the experience and **identify** usability issues.
  - 5. **Analyze**, aggregate, and present the **results**.

---

#### Define the scope

- On both large and small projects, **budgets** and other **resources** may be **limited**.
  - This may be especially the case on large eCommerce sites:
    - For example, it may not be feasible to examine the entire site, as it could take a very long time and therefore become too expensive.
  - This is where `scoping` the `heuristic analysis` comes in.
- `Scope`
  - What is in and what is out.
- Decisions may be made to **examine only the most crucial areas** of the site.
- The limited scope may only have the capacity to **focus on specific user flows and functionalities**, such as log in/register, search and browse, product detail pages, shopping cart, and checkout.

---

#### Know the business requirements and the users

- The evaluators should **understand the business needs** of the product/system.
- As with any typical user-centered design process, it’s crucial to **know the users**.
- To facilitate heuristic analysis, **specific user personas must be established.**
  - Are the end-users **novices** or **experts**?
  - What are the user demographics?
  - For example, although heuristics were meant to work as universal usability standards, perhaps special emphasis needs to be placed on **accessibility for an older audience**—or maybe diverse, multicultural audiences need to be kept in mind.

---

#### Decide which reporting tools and heuristics

- It’s incredibly important to decide which **set of heuristics** the evaluators are going to use.
  - A selected set of heuristics will provide **common guidelines** against which each of the experts can make their evaluation, as well as ensure that they are all on the same page.
  - **Without** it, the heuristic analysis process could fall into utter **chaos**
    - produce **inconsistent, conflicting reports** and ultimately become **ineffective**.
- As part of the heuristic evaluation plan, a system, a **format**, and which **tools** to use should be agreed upon.
  - This could be Google Docs, Sheets and Slides, or some other common reporting tool that everyone can use and to which the “observer” will have easy access.
- `Jakob Nielsen’s 10 Usability Heuristic`s for User Interface Design are probably the most commonly used set of usability heuristics.
- There are others such as the 2 lists that we have looked at previously

---

#### Evaluate the experience and usability issues

- When a heuristic evaluation is performed with a group of **experts**, each individual **evaluates the UI separately**.
  - This approach to the expert review is done in order to ensure the evaluations will be **independent and unbiased**.
- When all the evaluations are **complete**, the **findings** are then **collated** and **aggregated**.
  - In order to help the team move toward design solutions, findings must **describe** the issues **precisely**.
  - **Vague notes** such as “this layout will slow down the registration process” are not at all **productive** or of any value.
  - **Notes** need to be **specific and clearly identify** the heuristic that the issue violates.
    - For example: “During registration the UI layout is confusing, inconsistent and violates the rules of user control, feedback and consistency (#1, #20, and #16 respectively).”
- For the sake of speed, **visual** interfaces may be **printed out** and may be **marked up with notes** that can be consolidated later.
  - This method helps to quickly aggregate the expert’s final notes.
  - They can also be **coded** for easy identification by the design team.

---

#### Analyze, aggregate, and present the results.

- At the conclusion of a heuristic analysis, the **evaluation manager**—or observer—carries out some **housekeeping** and organization such as **removing duplicates** and **collating the findings**.
- The observer’s next step is to **aggregate** the heuristic evaluation reports and build a **table** that includes the **severity ratings** of usability issues and from which the design team can prioritize.
- “For usability testing to be valuable, study findings must clearly identify issues and help the team move toward design solutions.” – The Nielsen Norman Group

---

#### The analysis output

- The **output** from a heuristic analysis should be a **list of usability problems** that not only identify **specific problems**, but reference the **usability heuristics** the problems violate (preferable a code number for easy reference).
  - For example, the above screen points out that using low contrast text in the UI violates the heuristics of “visibility” and “discoverability.”

---

#### Analysis output

- Using **reference codes** from the chosen set of heuristics will help build a data table which can then be sorted.
- When the design team sees that a large number of issues reference a small number of violations (identified by code), they can focus their energies on improving them.
  - For example, there may be widespread issues of visibility and discoverability as in the previous example.
- Heuristic analysis **doesn’t** necessarily **provide fixes** to usability issues, nor does it provide a “success probability score” if the design improvements are to be implemented.
  - However, because a heuristic evaluation compares the UI against a set of known usability heuristics, in most cases it is **remarkably easy to identify the solution** to a specific problem and come up with a more compelling design.
