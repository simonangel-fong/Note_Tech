# AWS - S3

[Back](../index.md)

- [AWS - S3](#aws---s3)
  - [Amazon S3](#amazon-s3)
    - [Buckets](#buckets)
    - [Objects](#objects)
    - [Durability and Availability](#durability-and-availability)
    - [Hands-on](#hands-on)
  - [S3 Storage Classes](#s3-storage-classes)
    - [S3 Standard – General Purpose](#s3-standard--general-purpose)
    - [Infrequent Access](#infrequent-access)
    - [Glacier Storage Classes](#glacier-storage-classes)
    - [Intelligent-Tiering](#intelligent-tiering)
    - [Comparison](#comparison)
    - [Hands-on](#hands-on-1)
  - [Security](#security)
    - [Bucket Policies](#bucket-policies)
    - [Block Public Access](#block-public-access)
    - [Hands-On](#hands-on-2)
  - [Static Website Hosting](#static-website-hosting)
    - [Hands-on](#hands-on-3)
  - [Versioning](#versioning)
    - [Hands-on](#hands-on-4)
  - [Replication (CRR \& SRR)](#replication-crr--srr)
    - [Hands-on](#hands-on-5)
  - [Lifecycle Rules](#lifecycle-rules)
    - [Lifecycle Rules (Scenario 1)](#lifecycle-rules-scenario-1)
    - [Lifecycle Rules (Scenario 2)](#lifecycle-rules-scenario-2)
  - [`Storage Class Analysis`](#storage-class-analysis)
    - [Hands-on](#hands-on-6)
  - [Requester Pays](#requester-pays)
  - [Event Notifications](#event-notifications)
    - [IAM Permissions](#iam-permissions)
    - [Event Notifications with `Amazon EventBridge`](#event-notifications-with-amazon-eventbridge)
    - [Hands-on](#hands-on-7)
  - [Performance](#performance)
    - [Baseline Performance](#baseline-performance)
    - [Multi-Part upload](#multi-part-upload)
    - [S3 Transfer Acceleration](#s3-transfer-acceleration)
    - [S3 Byte-Range Fetches](#s3-byte-range-fetches)
  - [Select \& Glacier Select: filter](#select--glacier-select-filter)
  - [Batch Operations](#batch-operations)

---

## Amazon S3

- **Use cases**

  - Backup and storage
  - Disaster Recovery
  - Archive
  - Hybrid Cloud storage
  - Application hosting
  - Media hosting
  - Data lakes & big data analytics
  - Software delivery
  - Static website

---

### Buckets

- `Buckets`

  - as top level directories

- Bucket name:

  - must have a **globally unique** name (across all regions all accounts)
  - Naming convention
    - No uppercase, **No underscore**
    - 3-63 characters long
    - Not an IP
    - Must start with lowercase letter or number
    - Must NOT start with the prefix `xn--`
    - Must NOT end with the suffix `-s3alias`

- S3 looks like a global service but buckets are created in a region

  - Bucket **names** are defined in **golbal** level
  - **Buckets** are defined at the **region** level

---

### Objects

- `Objects`

  - the files stored in S3 buckets
  - Objects (files) have a **Key**

- `key`:

  - the FULL path of objects:

    - `s3://my-bucket/my_file.txt`
    - `s3://my-bucket/my_folder1/another_folder/my_file.txt`

  - There’s no concept of “directories” within buckets (although the UI will trick you to think otherwise)
  - Just keys with very long names that contain slashes (“/”)

  - composed of **prefix** + **object name**

![s3_object_key](./pic/s3_object_key.png)

- `Object value`

  - the content of the body
  - Max. Object Size is `5TB` (5000GB)
  - If uploading **more than 5GB**, must use **“multi-part upload”**
    - Multi-Part Upload is **recommended** as soon as the file is over `100 MB`.

- `Metadata`

  - list of text key / value pairs
  - system or user metadata

- `Tags`
  - Unicode key / value pair
  - up to `10`
  - useful for security / lifecycle
- `Version ID`
  - if versioning is enabled

---

### Durability and Availability

- **Durability:**

  - High durability (99.999999999%, 11 9’s) of objects across multiple AZ
  - If you store 10,000,000 objects with Amazon S3, you can on average expect to incur a **loss** of a single object once every 10,000 years
  - **Same** for all storage classes

- **Availability:**
  - Measures how **readily available** a service is
  - **Varies** depending on storage class
    - Example: S3 standard has 99.99% availability = not available 53 minutes a year

---

### Hands-on

- Create bucket

![s3_bucket_handson01](./pic/s3_bucket_handson01.png)

- Upload file

![s3_bucket_handson01](./pic/s3_bucket_handson02.png)

![s3_bucket_handson01](./pic/s3_bucket_handson03.png)

- Create folder

![s3_bucket_handson01](./pic/s3_bucket_handson04.png)

---

## S3 Storage Classes

### S3 Standard – General Purpose

- Used for **frequently accessed data**
- 99.99% Availability
- **Low latency** and high throughput
- Sustain 2 concurrent facility failures
- **Use Cases**:
  - Big Data analytics
  - mobile & gaming applications
  - content distribution…

---

### Infrequent Access

- For data that is **less frequently accessed**, but requires rapid access when needed
- Lower cost than S3 Standard

- **Amazon S3 Standard-Infrequent Access (S3 Standard-IA)**

  - 99.9% Availability
  - Use cases:
    - Disaster **Recovery**,
    - **backups**

- **Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA)**
  - High durability (99.999999999%) in a **single AZ**;
  - data **lost** when AZ is **destroyed**
  - 99.5% Availability
  - Use Cases:
    - Storing **secondary backup** copies of on-premises data, or data you can recreate

---

### Glacier Storage Classes

- **Low-cost** object storage meant for **archiving / backup**
- Pricing: price for **storage** + object **retrieval** cost

- **Amazon S3 Glacier Instant Retrieval**

  - Millisecond retrieval, great for data **accessed once a quarter**
  - Minimum storage duration of **90 days**

- **Amazon S3 Glacier Flexible Retrieval (formerly Amazon S3 Glacier):**

  - Types:
    - Expedited (1 to 5 minutes),
    - Standard (3 to 5 hours),
    - **Bulk** (5 to 12 hours) – **free**
  - Minimum storage duration of **90 days**

- **Amazon S3 Glacier Deep Archive – for long term storage:**
  - Standard (12 hours),
  - Bulk (48 hours)
  - Minimum storage duration of **180 day**

---

### Intelligent-Tiering

- Small **monthly monitoring** and auto-tiering fee
- Moves objects **automatically** between Access Tiers based on usage

- There are **no retrieval** charges in S3 Intelligent-Tiering
  - **Frequent** Access tier (automatic): **default** tier
  - **Infrequent** Access tier (automatic): objects not accessed for **30 days**
  - **Archive Instant** Access tier (automatic): objects not accessed for **90 days**
  - **Archive** Access tier (optional): configurable from 90 days to **700+** days
  - **Deep Archive** Access tier (optional): config. from **180** days to **700+** days

---

### Comparison

![s3_storage_class_comparison](./pic/s3_storage_class_comparison.png)

![s3_storage_price_comparison](./pic/s3_storage_price_comparison.png)

---

### Hands-on

- Upload file and select storage class.

![s3_storage_class_handson01](./pic/s3_storage_class_handson01.png)

![s3_storage_class_handson01](./pic/s3_storage_class_handson02.png)

- Create lifecycle rule

![s3_storage_class_handson01](./pic/s3_storage_class_handson03.png)

![s3_storage_class_handson01](./pic/s3_storage_class_handson04.png)

---

## Security

- **User-Based**

  - **IAM Policies**
    - which API calls should be **allowed for a specific user** from IAM

![s3_iam_permission_user_access_diagram](./pic/s3_iam_permission_user_access_diagram.png)

![s3_iam_role_ec2_access_diagram](./pic/s3_iam_role_ec2_access_diagram.png)

- **Resource-Based**

  - **Bucket Policies**
    - bucket wide rules from the S3 console
    - allows cross account

![s3_bucket_policy_public_access_diagram](./pic/s3_bucket_policy_public_access_diagram.png)

![s3_bucket_policy_cross_account_access_diagram](./pic/s3_bucket_policy_cross_account_access_diagram.png)

- **Object Access Control List (ACL)**
  - finer grain (can be disabled)
- **Bucket Access Control List (ACL)**

  - less common (can be disabled)

- Note: an IAM principal **can access** an S3 object if

  - The user **IAM permissions** `ALLOW` it **OR** the **resource policy** `ALLOWS` it
  - **AND** there’s **no** explicit `DENY`
    - Explicit DENY in an IAM Policy will take precedence over an S3 bucket policy.

- **Encryption**
  - encrypt objects in Amazon S3 using encryption keys

---

### Bucket Policies

- JSON based policies

  - **Resources**:
    - buckets and objects
  - **Effect**:
    - Allow / Deny
  - **Actions**:
    - Set of API to Allow or Deny
  - **Principal**:
    - The account or user to apply the policy to

![s3_bucket_policies_sample](./pic/s3_bucket_policies_sample.png)

- Use S3 bucket for policy to:
  - Grant public access to the **bucket**
  - Force objects to be **encrypted at upload**
  - Grant access to **another account** (Cross Account)

---

### Block Public Access

![s3_public_access](./pic/s3_public_access.png)

- These settings were created to **prevent company data leaks**
- If you know your bucket should never be public, leave these on
- **Can** be set **at the account level**

---

### Hands-On

1. Disable block public access

![s3_public_access_handson01](./pic/s3_public_access_handson01.png)

![s3_public_access_handson02](./pic/s3_public_access_handson02.png)

2. Edit Bucket policy, using **Policy generator**

![s3_public_access_handson03](./pic/s3_public_access_handson03.png)

![s3_public_access_handson04](./pic/s3_public_access_handson04.png)

![s3_public_access_handson05](./pic/s3_public_access_handson05.png)

![s3_public_access_handson06](./pic/s3_public_access_handson06.png)

---

## Static Website Hosting

- S3 can **host static websites** and have them **accessible on the Internet**
- The website URL will be (depending on the region)

  - `http://bucket-name.s3-website-aws-region.amazonaws.com` OR
  - `http://bucket-name.s3-website.aws-region.amazonaws.com`

- If you get a **403 Forbidden** error, make sure the **bucket policy allows public reads**!

![s3_static_website_hosting_diagram](./pic/s3_static_website_hosting_diagram.png)

---

### Hands-on

- Enable Website hosting

![s3_static_website_hosting_handson01](./pic/s3_static_website_hosting_handson01.png)

![s3_static_website_hosting_handson02](./pic/s3_static_website_hosting_handson02.png)

- Upload web files

- Website

![s3_static_website_hosting_handson03](./pic/s3_static_website_hosting_handson03.png)

---

## Versioning

- You can version your files in Amazon S3
- It is enabled **at the bucket level**
- **Same key overwrite** will change the “version”: 1, 2, 3….

- It is best practice to version your buckets

  - Protect against unintended deletes (ability to restore a version)
  - **Easy roll back** to previous version

- Notes:
  - Any file that is **not versioned prior to** enabling versioning will have **version `null`**
  - Suspending versioning does **not delete** the previous versions

![s3_versioning_diagram](./pic/s3_versioning_diagram.png)

---

### Hands-on

- Enable versioining

![s3_versioning_handson01](./pic/s3_versioning_handson01.png)

![s3_versioning_handson02](./pic/s3_versioning_handson02.png)

- Upload files

- Website after updated

![s3_versioning_handson03](./pic/s3_versioning_handson03.png)

![s3_versioning_handson04](./pic/s3_versioning_handson04.png)

- Roll back version: delete

![s3_versioning_handson05](./pic/s3_versioning_handson05.png)

![s3_versioning_handson06](./pic/s3_versioning_handson06.png)

- Delete: not real delete, but create a delete marker.
  - Disable "show version"
  - Select file to delete
  - Enable show version

![s3_versioning_handson07](./pic/s3_versioning_handson07.png)

- Website after deleted

![s3_versioning_handson07](./pic/s3_versioning_handson08.png)

---

## Replication (CRR & SRR)

- `S3 Replication`
  - allows you to **replicate data** from an S3 bucket to another **in the same/different AWS Region**.
- **Must enable Versioning** in source and destination buckets
- `Cross-Region Replication (CRR)`
- `Same-Region Replication (SRR)`

- Buckets can be in **different AWS accounts**
- Copying is **asynchronous**
- Must give proper **IAM permissions** to S3

- **Use cases**:
  - CRR: compliance, lower latency access, replication across accounts
  - SRR: log aggregation, live replication between production and test accounts

![s3_replication_diagram](./pic/s3_replication_diagram.png)

- After you enable Replication, **only new objects** are replicated
- Optionally, you can **replicate existing objects** using `S3 Batch Replication`

  - Replicates existing objects and objects that failed replication

- For **DELETE operations**

  - Can **replicate delete markers** from source to target (**optional** setting)
  - Deletions with a version ID(Permenant deletion) are **not replicated** (to avoid malicious deletes)

- There is **no “chaining”** of replication
  - If bucket 1 has replication into bucket 2, which has replication into bucket 3
  - Then objects created in bucket 1 are not replicated to bucket 3

---

### Hands-on

- Create 2 buckets

![s3_replication_handson01](./pic/s3_replication_handson01.png)

- Upload file to original bucket

![s3_replication_handson02](./pic/s3_replication_handson02.png)

- Create replication rule

![s3_replication_handson03](./pic/s3_replication_handson03.png)

![s3_replication_handson03](./pic/s3_replication_handson04.png)

![s3_replication_handson03](./pic/s3_replication_handson05.png)

- Upload file in origin bucket

![s3_replication_handson03](./pic/s3_replication_handson06.png)

- Target bucket will update the new uploaded file.
  - the file before replication will not copy.

![s3_replication_handson03](./pic/s3_replication_handson07.png)

- It is optional to replicate Delete marker

![s3_replication_handson03](./pic/s3_replication_handson08.png)

- If origin bucket deletes a file permanently, this deletion will be replicated to the target bucket.

---

## Lifecycle Rules

- `Lifecycle Rules`
  - used to move objects automatically.
  - You can transition objects between storage classes
    - For **infrequently accessed** object, move them to `Standard IA`
    - For **archive objects** that you don’t need fast access to, move them to `Glacier` or `Glacier Deep Archive`

![s3_lifecycle_rule_diagram01](./pic/s3_lifecycle_rule_diagram01.png)

- `Transition Actions`

  - configure objects to **transition to another storage class**
  - ie:
    - Move objects to Standard IA class **60 days after** creation
    - Move to Glacier for archiving **after 6 months**

- `Expiration actions`

  - configure objects to **expire (delete)** after some time
  - ie:
    - **Access log files** can be set to delete **after a 365 days**
    - Can be used to delete **old versions** of files (if versioning is enabled)
    - Can be used to delete **incomplete Multi-Part uploads**

- Rules can be created for a certain `prefix` (example: s3://mybucket/mp3/\*)
- Rules can be created for certain **objects Tags** (example: Department: Finance)

---

### Lifecycle Rules (Scenario 1)

- Requirement:

  - Your application on EC2 creates images thumbnails after profile photos are uploaded to Amazon S3. These thumbnails can be easily recreated, and only need to be kept for 60 days. The source images should be able to be immediately retrieved for these 60 days, and afterwards, the user can wait up to 6 hours. How would you design this?

- Solution:
  - S3 source images can be on `Standard`, with a lifecycle configuration to **transition** them to `Glacier` **after** 60 days
  - S3 thumbnails can be on `One-Zone IA`, with a lifecycle configuration to **expire** them (delete them) after 60 days

---

### Lifecycle Rules (Scenario 2)

- Requirement:

  - A rule in your company states that you should be able to recover your deleted S3 objects immediately for 30 days, although this may happen rarely. After this time, and for up to 365 days, deleted objects should be recoverable within 48 hours.

- Solution:
  - **Enable S3 Versioning** in order to have object versions, so that “deleted objects” are in fact hidden by **a “delete marker” and can be recovered**
  - Transition the “noncurrent versions” of the object to `Standard IA`
  - Transition afterwards the “noncurrent versions” to `Glacier Deep Archive`

---

## `Storage Class Analysis`

- `Storage Class Analysis`
  - Help you decide when to transition objects to the right storage class
  - Recommendations for `Standard` and `Standard IA`
    - Does NOT work for `One-Zone IA` or `Glacier`
  - Report is updated daily
  - 24 to 48 hours to start seeing data analysis
  - Good first step to put together Lifecycle Rules (or improve them)!

![s3_storage_class_analysis](./pic/s3_storage_class_analysis.png)

---

### Hands-on

![s3_lifecycle_rule_handson01](./pic/s3_lifecycle_rule_handson01.png)

![s3_lifecycle_rule_handson02](./pic/s3_lifecycle_rule_handson02.png)

![s3_lifecycle_rule_handson03](./pic/s3_lifecycle_rule_handson03.png)

![s3_lifecycle_rule_handson04](./pic/s3_lifecycle_rule_handson04.png)

![s3_lifecycle_rule_handson05](./pic/s3_lifecycle_rule_handson05.png)

![s3_lifecycle_rule_handson06](./pic/s3_lifecycle_rule_handson06.png)

![s3_lifecycle_rule_handson07](./pic/s3_lifecycle_rule_handson07.png)

![s3_lifecycle_rule_handson08](./pic/s3_lifecycle_rule_handson08.png)

---

## Requester Pays

- In general, **bucket owners** pay for all Amazon S3 **storage** and **data transfer costs** associated with their bucket

- With `Requester Pays` buckets, the **requester** instead of the bucket owner **pays the cost of the request and the data download from the bucket**

- Helpful when you want to **share large datasets** with **other accounts**

- The **requester must be authenticated** in AWS (cannot be anonymous)

![s3_requester_pay_diagram](./pic/s3_requester_pay_diagram.png)

---

## Event Notifications

- `Event`:

  - S3:ObjectCreated, S3:ObjectRemoved, S3:ObjectRestore, S3:Replication…
  - Object name **filtering** possible (\*.jpg)
  - Can create **as many** “S3 events” as desired

- S3 event notifications typically deliver events in seconds but can sometimes take a minute or longer

- Use case:
  - generate thumbnails of images uploaded to S3

![s3_event_notification_diagram](./pic/s3_event_notification_diagram.png)

---

### IAM Permissions

- Instead of using role, defining access policy
  - SNS Resournce Policy(Publish): allow s3 bucket to send messages directly into the SNS topic.
  - SQS Resournce Policy(SendMessage): allow s3 bucket to send data directly into the SQS queue.
  - Lambda Resournce Policy(InvokeFunction): allow s3 bucket to invoke lambda function.

![s3_event_notification_IAM_diagram](./pic/s3_event_notification_IAM_diagram.png)

---

### Event Notifications with `Amazon EventBridge`

- Integration:

![s3_event_notification_amazon_eventbridge_diagram](./pic/s3_event_notification_amazon_eventbridge_diagram.png)

- Benefits:
  - **Advanced filtering options** with JSON rules (metadata, object size,
    name...)
  - **Multiple Destinations**
    - ex Step Functions, Kinesis Streams / Firehose…
  - **EventBridge Capabilities**
    - Archive, Replay Events, Reliable delivery

---

### Hands-on

- Create SQS

![s3_event_notification_handson_SQS01](./pic/s3_event_notification_handson_SQS01.png)

![s3_event_notification_handson_SQS02](./pic/s3_event_notification_handson_SQS02.png)

![s3_event_notification_handson_SQS03](./pic/s3_event_notification_handson_SQS03.png)

![s3_event_notification_handson_SQS04](./pic/s3_event_notification_handson_SQS04.png)

![s3_event_notification_handson_SQS05](./pic/s3_event_notification_handson_SQS05.png)

![s3_event_notification_handson_SQS06](./pic/s3_event_notification_handson_SQS06.png)

![s3_event_notification_handson_SQS07](./pic/s3_event_notification_handson_SQS07.png)

![s3_event_notification_handson_SQS08](./pic/s3_event_notification_handson_SQS08.png)

-

![s3_event_notification_handson01](./pic/s3_event_notification_handson01.png)

![s3_event_notification_handson02](./pic/s3_event_notification_handson02.png)

![s3_event_notification_handson03](./pic/s3_event_notification_handson03.png)

![s3_event_notification_handson04](./pic/s3_event_notification_handson04.png)

![s3_event_notification_handson05](./pic/s3_event_notification_handson05.png)

- Test in SQS

![s3_event_notification_handson07](./pic/s3_event_notification_handson07.png)

![s3_event_notification_handson08](./pic/s3_event_notification_handson08.png)

- Upload object

![s3_event_notification_handson09](./pic/s3_event_notification_handson09.png)

- SQS

![s3_event_notification_handson10](./pic/s3_event_notification_handson10.png)

---

## Performance

### Baseline Performance

- Amazon S3 automatically scales to high request rates, latency 100-200 ms
- Your application can achieve at least **3,500** `PUT/COPY/POST/DELETE` or **5,500** `GET/HEAD` requests **per second per prefix** in a bucket.

- There are **no limits** to the **number of prefixes** in a bucket.
- Example (object path => prefix):

  - bucket/**folder1/sub1**/file => /**folder1/sub1**/
  - bucket/**folder1/sub2**/file => /**folder1/sub2**/
  - bucket/**1**/file => /**1**/
  - bucket/**2**/file => /**2**/

- If you spread reads across all four prefixes **evenly**, you can achieve 22,000
  requests per second for `GET` and `HEAD`

---

### Multi-Part upload

- **recommended** for files **> 100MB**,
- **must use** for files **> 5GB**
- Can help parallelize uploads (speed up transfers)

![s3_multi-part-upload](./pic/s3_multi-part-upload.png)

---

### S3 Transfer Acceleration

- Increase transfer speed by transferring file to an `AWS edge location` which will **forward** the data to the S3 bucket in the **target region**

- Compatible with multi-part upload

![s3_transfer_acceleration](./pic/s3_transfer_acceleration.png)

---

### S3 Byte-Range Fetches

- **Parallelize** `GETs` by requesting specific byte ranges
- Better resilience in case of failures

- Can be used to speed up downloads

![s3_performance_fetch01](./pic/s3_performance_fetch01.png)

- Can be used to **retrieve only partial data** (for example the head of a file)

![s3_performance_fetch02](./pic/s3_performance_fetch02.png)

---

## Select & Glacier Select: filter

- Retrieve less data using `SQL` by performing **server-side filtering**
- Can filter **by rows & columns** (simple SQL statements)
- **Less** network transfer, less CPU cost client-side

![s3_select_glacier_select](./pic/s3_select_glacier_select.png)

---

## Batch Operations

- `Batch Operations`

  - manages retries, tracks progress, sends completion notifications, generate reports …

- Perform bulk operations on existing S3 objects with a single request, example:

  - Modify object **metadata** & **properties**
  - **Copy objects** between S3 buckets
  - **Encrypt un-encrypted objects**
  - Modify **ACLs**, **tags**
  - **Restore** objects from `S3 Glacier`
  - **Invoke Lambda function** to perform custom action on each object

- A `job` consists of

  - a `list of objects`,
  - the `action` to perform,
  - and **optional** `parameters`

- You can use
  - `S3 Inventory` to **get object list**
  - `S3 Select` to **filter** your objects

![s3_batch_operation](./pic/s3_batch_operation.png)

---

[TOP](#aws---s3)
