# WK 10 - Social Media & The Dark Web

[Back](../index.md)

- [WK 10 - Social Media \& The Dark Web](#wk-10---social-media--the-dark-web)
  - [Social Media](#social-media)
  - [Terms of Service](#terms-of-service)
  - [Social Media Attacks](#social-media-attacks)
  - [Mobile Users](#mobile-users)
  - [Clearnet vs. Deep Web vs. Darknet](#clearnet-vs-deep-web-vs-darknet)
    - [Robots.txt](#robotstxt)
    - [Darknet](#darknet)
      - [best practice](#best-practice)

---

## Social Media

## Terms of Service

- Companies usually **reserve the right to change** their privacy policy settings without prior consent of the user
- Your privacy settings on a social media platform may be reset when such changes are applied
- Might go back to default settings (less secure)
- Do you find it easy to change your privacy settings on Facebook?

---

## Social Media Attacks

## Mobile Users

## Clearnet vs. Deep Web vs. Darknet

- `Clearnet`

  - the regular “internet”
  - When you search for sites on Google, you will be presented with results from the `Clearnet`
  - The websites on the Clearnet are meant to be found through the use of regular `search engines` as their **spiders** are able to index all of the content

- `Deepweb`

  - are either unavailable due to **authentication**, or have been **restricted** by the web servers,
  - the part of the internet that is typically **hidden** from or inaccessible by regular users
  - Regular internet users are only able to access the “tip of the iceberg”
  - Search engines such as **Google** will **not show** results for Deepweb sites
  - Any **page that requires authentication** is not accessible by Google’s spiders
    - Your private banking information
    - Hidden Facebook profiles
    - Deactivated dating profiles
    - Your Private webmail account
  - It is estimated that **95%** of the resources online are considered to be in the Deepweb

### Robots.txt

- `Robots.txt` / `Robots Exclusion Standard or Protocol`

  - one way of letting any web spider (Google, Yahoo, etc.) know that a page **should not be indexed**
  - It works by specifying which directories and/or files are to be **accessible** within a web server’s directory structure to specified robots
  - can **specify which crawler** you want to access what resources

- Example:

  - `Allow: /emaildirectory/contactus.html`
  - `Disallow: /emaildirectory/`

- **Different search engines** support different components of this de facto standard
- Although most major search engines will adhere to the robots.txt file, they may not acknowledge some of the contents within them like the allow directive

- Although these methods allow Search Engines to crawl and index different resources and information, it **only works for legitimate crawlers**
- **Malicious crawlers** will typically go to the robots.txt file right away, and start from there
- Most legitimate sites can be **compromised** or may **find vulnerabilities** in the pages and files listed as disallow in the document

---

### Darknet

- Although much of the `Deepweb` is **inaccessible** to web **crawlers/spiders**, that still makes up a large portion of the internet that is **available to Search Engines** like Google **but restricted** due to authentication

- `Darknet / Darkweb`

  - most of these sites are **“hidden”** from the rest of the internet

- `The Onion Router (TOR)`

  - a browser specifically designed to **access resources located on the Darknet**

- It was originally designed by the **U.S. Navy** to conceal communications
- Websites accessible by `TOR` include `.onion` domains

- The **TOR network** uses `nodes` to route traffic

  - Each node **only decrypts** the information it **needs to route** the traffic further towards it’s next destination
  - This is supposed to **prevent the node from knowing the source or destination** of the traffic
  - An `exit node` is used at the **end** to send the traffic to it’s **final destination**

- There is an **inherent issue** with the fact that anyone can run an `exit node` and read the traffic
- Users can work around this issue by using the `TOR browser` (a modified version of Firefox that attempts to use HTTPS for all traffic)
- Another **layer of security** is using a `VPN tunnel`
  - This is only secure if you can trust the **VPN provider** to not log your activity

---

#### best practice

- In addition to the network, it is best practice to include the following when accessing the darknet:
- A “reasonably” secure Operating O/S
  - Qubes OS
  - Tails OS
  - Kali Linux OS
- Using a Virtual Machine
- Anti-Virus

- Most of the activities on the TOR network are just **users** trying to be **anonymous**
- Keeping that in mind, there are also activities on the dark web that are **illegal**

  - Drug sales
  - Illegal Pornography
  - Weapon sales
  - Etc.

- Install `tor`: `apt-get install tor`

- **Hidden Services**

  - These allow for **two-way anonymity**
  - The `Client` and the `Server` do **not know each other’s IP address**
  - Servers will have a `.onion` hostname and all the **traffic will be routed across the TOR network**

- These services should be **listening only** on a `localhost` address (`127.0.0.1`)
- Access from the **internet** should **not be available**

- Servers hosting hidden services should **not disclose any identifying information** such as software type or version
- If any other resources are being accessed by the server (DNS, etc.), ensure that all that traffic is **also routed through the TOR network**
  - `IPtables` to force traffic routing
  - `TOR SOCKS Proxy`

- Security and Configuration
  - **Multiple ports** can be used for each hidden (onion) service
  - `SSL/TLS` is **not required**, but can add to the layers of the onion
  - `Egress filtering` should be applied to ensure that no external connections are allowed
  - Onion services should **not be hosted** on one machine for **too long** as patterns can be enumerated
  - `OnionScan` can be used to check for information leaks

- Server location exposure
  - **Error messages** and the **HTTP referer headers** may **expose the location of a server**
  - Attackers may use `Server Side Request Forgery (SSRF)` attacks to perform external connections such as a DNS lookup to **expose the server’s location**
  - **Bad relays** in the TOR network may **expose a server’s location**