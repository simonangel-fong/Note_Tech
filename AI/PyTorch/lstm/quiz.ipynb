{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quiz\n",
    "\n",
    "- Name: Wenhao Fang\n",
    "- ID: N01555914\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import torch\n",
    "\n",
    "DATA = [0.1, 0.85, 0.55, 0.45, 0.2, 0.3, 0.4, 0.6, 0.01]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement model\n",
    "\n",
    "- using the parameter shown in the diagram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "\n",
    "class LSTM:\n",
    "    # initial\n",
    "    def __init__(self, init_ltm, init_stm):\n",
    "        self.ltm = init_ltm\n",
    "        self.stm = init_stm\n",
    "\n",
    "    def forward(self, x):\n",
    "        # forget block\n",
    "        self.ltm = torch.sigmoid(torch.tensor((1.63*x+2.70*self.stm)))\n",
    "        # input block\n",
    "        # left block + right block\n",
    "        # sigmoid +  relu\n",
    "        self.ltm = torch.sigmoid(torch.tensor(\n",
    "            1.65*x+4.38*self.stm+0.62)) + torch.relu(torch.tensor(0.94*x+1.41*self.stm - 0.32))\n",
    "\n",
    "        # # output block\n",
    "        # # sigmoid, relu\n",
    "        self.stm = torch.sigmoid(torch.tensor(\n",
    "            (-0.19*x + 2*self.stm+0.59) + torch.relu(self.ltm)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 10\n",
      "LTM: 2.08\n",
      "STM: 0.99 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\simon\\AppData\\Local\\Temp\\ipykernel_12704\\2586623465.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.stm = torch.sigmoid(torch.tensor(\n",
      "C:\\Users\\simon\\AppData\\Local\\Temp\\ipykernel_12704\\2586623465.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.ltm = torch.sigmoid(torch.tensor((1.63*x+2.70*self.stm)))\n",
      "C:\\Users\\simon\\AppData\\Local\\Temp\\ipykernel_12704\\2586623465.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.ltm = torch.sigmoid(torch.tensor(\n",
      "C:\\Users\\simon\\AppData\\Local\\Temp\\ipykernel_12704\\2586623465.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  1.65*x+4.38*self.stm+0.62)) + torch.relu(torch.tensor(0.94*x+1.41*self.stm - 0.32))\n"
     ]
    }
   ],
   "source": [
    "# predict\n",
    "\n",
    "INIT_LTM = 0\n",
    "INIT_STM = 0\n",
    "\n",
    "model = LSTM(init_ltm=INIT_LTM, init_stm=INIT_STM)\n",
    "num = 1\n",
    "for x in DATA:\n",
    "    num += 1\n",
    "    model.forward(x)  # pass data to model\n",
    "\n",
    "print(f\"Time: {num}\\nLTM: {model.ltm:.2f}\\nSTM: {model.stm:.2f} \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
